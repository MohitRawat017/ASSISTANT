# V1 Desktop Voice Assistant (Push-to-Talk)

A local, push-to-talk desktop AI assistant that listens on demand, understands spoken or typed commands, responds with low-latency speech, and performs basic desktop actions like opening applications or searching. This is V1 â€” deliberately scoped, safe, and stable.

## ğŸš€ Project Goals (V1)
This project aims to build a personal AI co-pilot, not a fully autonomous agent.

### âœ… What V1 DOES
- **Push-to-activate**: Uses a hotkey (F2) for listening.
- **Speech Recognition**: English only, optimized for low latency.
- **Text Fallback**: Supports manual text command input.
- **Fast Responses**: Spoken responses with selectable voices.
- **Desktop Control**: Open apps, search files, open URLs, and focus applications.

### âŒ What V1 DOES NOT
- Always-on wake word activation.
- Autonomous workflows or multi-step UI automation.
- File deletion or system modification.
- Continuous screen monitoring.

---

## ğŸ§© Core Features

### 1ï¸âƒ£ Push-to-Talk Activation
- Press **F2** to activate listening.
- Press **F2** again or use "cancel" keywords to interrupt.
- No always-on microphone for improved privacy.

### 2ï¸âƒ£ Speech Recognition (ASR)
- Converts spoken English to text using open-source Hugging Face models.
- Optimized for speed and low-latency interaction.

### 3ï¸âƒ£ Text-to-Speech (TTS)
- Streaming playback for minimal perceived delay.
- Customizable voice selection via configuration.

### 4ï¸âƒ£ Desktop Control
- Supported actions:
    - Open applications (e.g., Chrome, Spotify).
    - Search for files or apps.
    - Open URLs in the default browser.
    - Bring specific applications to focus.
- **Constraint**: All actions are explicit, single-step, and non-destructive.

---

## ğŸ— Architecture Overview

```mermaid
graph TD
    A[F2 Press / Text Input] --> B{Input Type}
    B -->|Audio| C[Audio Capture]
    B -->|Text| D[NLP Input]
    C --> E[ASR Pipeline]
    E --> F[LLM Reasoning + Tools]
    D --> F
    F --> G{Action Required?}
    G -->|Yes| H[OS-Level Control]
    G -->|No| I[Text Response]
    H --> I
    I --> J[Text-to-Speech]
    J --> K[Audio Playback]
```

### Key Design Principles
- **Event-driven**: Not always listening.
- **Modular**: Discrete components for ASR, LLM, TTS, and Control.
- **Safety First**: Explicit tool calls and non-destructive actions.

---

## ğŸ“‚ Project Structure

```text
your_assistant_project/
â”œâ”€â”€ models/                     # Optional: cached local models
â”‚   â”œâ”€â”€ asr/
â”‚   â”œâ”€â”€ llm/
â”‚   â””â”€â”€ tts/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                 # Entry point & state machine
â”‚   â”œâ”€â”€ audio_input/            # Capture & ASR (mic_listener, asr, vad)
â”‚   â”œâ”€â”€ nlp/                    # LLM handlers & prompt templates
â”‚   â”œâ”€â”€ audio_output/           # TTS & playback
â”‚   â”œâ”€â”€ desktop_control/        # OS-specific actions
â”‚   â”œâ”€â”€ utils/                  # Logger & configuration
â”‚   â””â”€â”€ ui/                     # Optional text UI
â”œâ”€â”€ tests/                      # Unit and integration tests
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .env
```

---

## ğŸ§  State Machine (V1)
The assistant transitions between the following states:
`IDLE` â” (F2) â” `LISTENING` â” (Capture) â” `THINKING` â” (Tool Call) â” `ACTING` â” `SPEAKING` â” `IDLE`

*Interruptions (F2 or cancel keywords) reset the state immediately.*

---

## âš¡ Latency Strategy
Target perceived response time: **600â€“800 ms**.
- Push-to-talk avoids background noise processing.
- Streaming ASR and TTS playback minimize wait times.
- Early response chunking for faster verbal feedback.

---

## ğŸ›¡ Safety & Guardrails
- **Desktop Actions**: Explicit tool invocation only; no free-form execution.
- **Ambiguous Requests**: If intent is unclear, the assistant will state: *"I'm sorry, I am incapable of doing that."* to reduce hallucinations.

---

## ğŸ§ª Example Commands
- â€œOpen Chromeâ€
- â€œSearch for my resumeâ€
- â€œOpen Spotifyâ€
- â€œWhat time is it?â€
- â€œCancelâ€ / â€œForget itâ€

---

## âš™ Configuration & Dependencies
- **Settings**: All configurations (models, voices, hotkeys) live in `src/utils/config.py`.
- **Requirements**: Python 3.10+, Hugging Face Transformers, PyAudio/sounddevice, OS automation libraries, and Torch.

---

## ğŸš§ Known Limitations (V1)
- English only.
- Single-turn commands (no memory persistence).
- No continuous screen awareness or complex UI navigation.

---

## ğŸ›£ Roadmap (V2)
- Wake-word activation.
- Screen understanding and smarter disambiguation.
- Short & long-term memory.
- Multi-step workflows.

---

## ğŸ§  Philosophy
This assistant is designed as a **reliable co-pilot**, not an autonomous pilot. Stability, safety, and responsiveness are prioritized over complexity.
